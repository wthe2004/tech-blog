---
{"publish":true,"created":"2025-09-24T12:16:21.444-04:00","modified":"2025-09-24T20:58:56.220-04:00","tags":["ai","math"],"cssclasses":""}
---

### 前言：什么是梯度


简单来说，神经网络的训练过程就是通过优化算法（如梯度下降法）来调整网络中的参数（权重 $W$ 和偏置 $b$），以最小化一个**损失函数（Loss Function）$L$**。

梯度就是损失函数 $L$ 相对于某个参数（比如权重 $W$）的**偏导数** $\frac{\partial L}{\partial W}$。这个导数告诉我们：

  * **方向**：朝着哪个方向调整参数 $W$，损失函数 $L$ 的值会增长得最快。
  * **大小**：参数 $W$ 的微小变动，会在多大程度上影响损失函数 $L$。

在训练时，我们让参数向着梯度的**反方向**更新，这样就能最高效地减小损失。更新规则如下：

$W_{new} = W_{old} - \eta \frac{\partial L}{\partial W_{old}}$$

其中 $\eta$ 是学习率。


**关键点**：如果梯度 $\frac{\partial L}{\partial W}$ 非常非常小，接近于0，那么 $\eta \frac{\partial L}{\partial W_{old}}$ 也会接近于0。这意味着每次更新对参数 $W$ 的改动微乎其微，网络几乎学不到任何东西。这就是**梯度消失**。

-----

### 梯度消失的数学根源：链式法则
梯度消失的根本原因在于深度神经网络的反向传播（Backpropagation）过程，其核心是**链式法则（Chain Rule）**。


### 链式法则



我们来看一个简化的深度神经网络结构，假设有 $n$ 层：

  * $x$ 是输入，$y$ 是真实标签。
  * $z_i = W_i a_{i-1} + b_i$ 是第 $i$ 层的线性计算结果（其中 $a_0 = x$）。
  * $a_i = \sigma(z_i)$ 是第 $i$ 层的激活输出，$\sigma$ 是激活函数。
  * $L$ 是最终的损失函数。

我们的目标是计算损失函数 $L$ 对第一层权重 $W\_1$ 的梯度 $\frac{\partial L}{\partial W_1}$，以便更新它。根据链式法则，这个梯度可以被分解为一系列偏导数的乘积：

$\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial a_n} \cdot \frac{\partial a_n}{\partial z_n} \cdot \frac{\partial z_n}{\partial a_{n-1}} \cdot \frac{\partial a_{n-1}}{\partial z_{n-1}} \cdot \ldots \cdot \frac{\partial a_2}{\partial z_2} \cdot \frac{\partial z_2}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} \cdot \frac{\partial z_1}{\partial W_1}$$

我们来分析这个长长的乘法链中的关键项：

1.  $\frac{\partial z_i}{\partial a_{i-1}} = W_i$
2.  $\frac{\partial a_i}{\partial z_i} = \sigma'(z_i)$ (激活函数的导数)

将它们代入上面的链式法则表达式，我们可以简化得到：

$\frac{\partial L}{\partial W_1} = \left( \frac{\partial L}{\partial a_n} \cdot \frac{\partial z_1}{\partial W_1} \right) \cdot \prod_{i=2}^{n} \left( \sigma'(z_i) \cdot W_i \right)$$



这个公式是梯度消失问题的核心。梯度 $\frac{\partial L}{\partial W_1}$ 是一个**连乘**的结果。

### 两大“罪魁祸首”

导致这个连乘积趋近于零的罪魁祸首有两个：

#### 1\. 激活函数的导数 $\sigma'(z)$

在深度学习早期，**Sigmoid** 函数非常流行。它的公式是 $\sigma(x) = \frac{1}{1 + e^{-x}}$。

让我们看看它的导数 $\sigma'(x) = \sigma(x)(1 - \sigma(x))$ 的图像：

![[20 - Concepts/sigmoid-derivative.png]]

从图中可以看出：

  * Sigmoid 函数导数的**最大值是 0.25**（在 $z=0$ 时取得）。
  * 当输入 $z$ 的绝对值稍微大一些（例如 $|z| > 4$），其导数就趋近于 0。

在反向传播的连乘公式中，我们有很多个 $\\sigma'(z\_i)$ 项。即使每个 $\\sigma'(z\_i)$ 都取到最大值 0.25，经过多层传播后，梯度也会被指数级地衰减。例如，经过10层，梯度就会被乘以 $(0.25)^{10}$，这是一个极小的数字。

$$\text{Gradient} \propto \prod_{i} \sigma'(z_i) \implies \text{Gradient} \propto (\text{number} \le 0.25)^{\text{many layers}} \to 0$$

#### 2\. 权重的初始值 $W_i$

通常，为了防止梯度爆炸，我们会将权重 $W$ 初始化为接近于 0 的小数（例如，均值为0，方差为1或更小的高斯分布）。

如果权重 $|W_i|$ 的值普遍小于 1，那么在连乘公式 $\prod_{i=2}^{n} (\sigma'(z_i) \cdot W_i)$ 中，每一项都是一个小于 1 的数乘以另一个小于 1 的数，结果会变得更小。

**总结**：在反向传播过程中，梯度从输出层向输入层传播。每经过一层，它都要乘以一个**小于1的因子**（主要是激活函数的导数）。网络越深，这个连乘链就越长，导致传到浅层网络（靠近输入的层）的梯度变得极其微小，几乎为零。

### 梯度消失的后果

  * **浅层网络学习停滞**：靠近输入层的参数（如 $W_1, W_2$）几乎得不到更新，因为它们的梯度太小了。这导致这些层无法学习到有效的特征表示。
  * **训练时间极长**：模型需要非常长的时间才能让浅层网络开始学习，甚至可能永远无法有效训练。
  * **深度网络的优势丧失**：我们构建深度网络的初衷是学习从低级到高级的复杂特征。如果浅层网络无法学习，整个深度结构就失去了意义。

-----

### 如何从数学上缓解梯度消失？

理解了梯度消失的数学根源后，解决方案也就变得清晰了，核心思想是**避免连乘项持续小于1**。

改进方案：
1. [[20 - Concepts/ReLU]]
2. [[20 - Concepts/He初始化]]
3. [[ResNet]]
4. [[Batch Normalization]]
