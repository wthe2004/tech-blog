---
{"publish":true,"created":"2025-09-24T12:48:35.914-04:00","modified":"2025-09-24T20:58:46.926-04:00","tags":["math","ai"],"cssclasses":""}
---

### 链式法则



我们来看一个简化的深度神经网络结构，假设有 $n$ 层：

  * $x$ 是输入，$y$ 是真实标签。
  * $z_i = W_i a_{i-1} + b_i$ 是第 $i$ 层的线性计算结果（其中 $a_0 = x$）。
  * $a_i = \sigma(z_i)$ 是第 $i$ 层的激活输出，$\sigma$ 是激活函数。
  * $L$ 是最终的损失函数。

我们的目标是计算损失函数 $L$ 对第一层权重 $W\_1$ 的梯度 $\frac{\partial L}{\partial W_1}$，以便更新它。根据链式法则，这个梯度可以被分解为一系列偏导数的乘积：

$$\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial a_n} \cdot \frac{\partial a_n}{\partial z_n} \cdot \frac{\partial z_n}{\partial a_{n-1}} \cdot \frac{\partial a_{n-1}}{\partial z_{n-1}} \cdot \ldots \cdot \frac{\partial a_2}{\partial z_2} \cdot \frac{\partial z_2}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} \cdot \frac{\partial z_1}{\partial W_1}$$

我们来分析这个长长的乘法链中的关键项：

1.  $\frac{\partial z_i}{\partial a_{i-1}} = W_i$
2.  $\frac{\partial a_i}{\partial z_i} = \sigma'(z_i)$ (激活函数的导数)

将它们代入上面的链式法则表达式，我们可以简化得到：

$$\frac{\partial L}{\partial W_1} = \left( \frac{\partial L}{\partial a_n} \cdot \frac{\partial z_1}{\partial W_1} \right) \cdot \prod_{i=2}^{n} \left( \sigma'(z_i) \cdot W_i \right)$$

