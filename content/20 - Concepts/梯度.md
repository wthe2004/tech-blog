---
{"publish":true,"created":"2025-09-24T12:38:24.600-04:00","modified":"2025-09-24T20:58:23.032-04:00","tags":["ai","math"],"cssclasses":""}
---

简单来说，神经网络的训练过程就是通过优化算法（如梯度下降法）来调整网络中的参数（权重 $W$ 和偏置 $b$），以最小化一个**损失函数（Loss Function）$L$**。

梯度就是损失函数 $L$ 相对于某个参数（比如权重 $W$）的**偏导数** $\frac{\partial L}{\partial W}$。这个导数告诉我们：

  * **方向**：朝着哪个方向调整参数 $W$，损失函数 $L$ 的值会增长得最快。
  * **大小**：参数 $W$ 的微小变动，会在多大程度上影响损失函数 $L$。

在训练时，我们让参数向着梯度的**反方向**更新，这样就能最高效地减小损失。更新规则如下：

$$W_{new} = W_{old} - \eta \frac{\partial L}{\partial W_{old}}$$

其中 $\eta$ 是学习率。
